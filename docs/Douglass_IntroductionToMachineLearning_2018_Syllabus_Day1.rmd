---
title: "Introduction to Machine Learning (Syllabus/Code for Day 1): Information Theory and Problems in Learning"
output: 
  html_notebook:
    toc: true # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
    highlight: tango  # specifies the syntax highlighting style
    toc_float: true
---


```{css}

pre code, pre, code {
  white-space: pre !important;
  overflow-x: !scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

code.r{
  overflow-x: !scroll !important;
}

```

Rex W. Douglass<br/>
Director Machine Learning for Social Science Lab (MSSL)<br/>
Center for Peace and Security Studies (cPASS)<br/>
Department of Political Science<br/>
University of California San Diego<br/>
rexdouglass@gmail.com<br/>
www.rexdouglass.com<br/>
@rexdouglass 

# Course Overview

Please bring a two sided coin(s) and scratch paper to class for passing notes for the demonstrations.

This is a 6 hour introduction to machine learning spread across two three-hour lectures. The goal of this very short course is narrow: to give you enough of an overview, vocabulary, and intuition, so that you can identify machine learning problems in the wild and begin your own research into relevant literatures and possible approaches. The goal is not to train you to execute a particular machine learning solution. There are far too many approaches available; they may not cover whatever problem you find; and the state of the art will be different in a year or two anyway. Instead, we will learn how to think about and classify problems into broad types, how to define and measure the efficacy of different solutions to that problem, how to avoid some common and subtle mistakes, and how to think about a full machine learning pipeline from start to finish.


## Course Slides
* [Course Slides (1A)](https://docs.google.com/presentation/d/19i2om_jwK8m3a-jNvgtM-WMT1l1HAGaGuWeb4bgLsTM/edit?usp=sharing)
* [Course Slides (1B)](https://docs.google.com/presentation/d/1Z857fFS692ijppXZzrPVjsVxsllwDKzoFBWOchRjDfU/edit?usp=sharing)
* [Course Slides (2A)](https://docs.google.com/presentation/d/1HRzRTjz31vt_HwOkKE_jNUYhg1a2LrOxS6LM3RI9dE4/edit?usp=sharing)
* [Course Slides (2B)](https://docs.google.com/presentation/d/1GSKQeoYWTVlIfWQIV9pXyoZW3DVdL2ylmaUU5dN9sGA/edit?usp=sharing)

## Readings Policy
Math and programming are not something you learn, they're something you get used to. The readings of this course are, with a few exceptions, voluntary and intended for self study. They are to help point you in the right direction when you realize you need to start brushing up on a particular set of tools in order to tackle a particular problem.

## Textbooks

Do not purchase any books - each of these should be available for free on-line at the link given. Any individual one would provide a decent background to the field of machine learning. For this course, I've picked select chapters when I thought they did a good job reviewing a specific subtopic.

* (CIML)  [A course in machine learning](ciml.info/), Hal Daume III
* (ESL) [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/), Trevor Hastie and Robert Tibshirani
* (ISL) [An introduction to statistical learning: with application in R, Gareth James](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), Daniela Witten, Trevor Hastie, and Robert Tibshirani
* (IML) [Introduction to Machine Learning](http://alex.smola.org/drafts/thebook.pdf), Alex Smola and S.V.N. Vishwanathan
* (IntroMachineLearningWithR) ["An Introduction to Machine Learning with R"](https://lgatto.github.io/IntroMachineLearningWithR/index.html),Laurent Gatto, 2017-10-18
* (ML) [Machine Learning: The art and science of algorithms that make sense of data](http://dsd.future-lab.cn/members/2015nlp/Peter_Flach_Machine_Learning._The_Art_and_Scienc(BookZZ.org).pdf), Flach
* (MLPP) [Machine Learning: A Probabilistic Perspective](https://www.cs.ubc.ca/~murphyk/MLbook/), Kevin Murphy
* (PRML) [Patter recognition and machine learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf), Christopher M. Bishop
* (WMLW) ["WHY MACHINE LEARNING WORKS"](http://www.cs.cmu.edu/~gmontane/montanez_dissertation.pdf), George D. Montanez, May 2017, Dissertation

## General Resources

Related Classes
*[COMS W4721 Machine Learning for Data Science](http://www.columbia.edu/~jwp2128/Teaching/W4721/Spring2017/W4721Spring2017.html)

There are a number places on-line for constant updates on machine learning <br/>
* [Reddit Machine Learning Subreddit](https://www.reddit.com/r/MachineLearning/)<br/>
* [arxiv](https://twitter.com/arxiv_org)<br/>
* [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/)<br/>
* [My twitter feed](https://twitter.com/RexDouglass)<br/>
* [Political Analysis](https://www.cambridge.org/core/journals/political-analysis)<br/>
* [openreview](https://openreview.net/)<br/>
* [Distill](https://distill.pub/)<br/>

Conferences <br/>
* [Top Conferences for Machine Learning & Arti. Intelligence](http://www.guide2research.com/topconf/machine-learning) <br/>
* [Neural Information Processing Systems (NIPS)](https://nips.cc/) <br/>
* [International Conference on Machine Learning](https://icml.cc/) <br/>

## Data Archives
* [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php)<br/>
* [Kaggle Datasets](https://www.kaggle.com/datasets?sortBy=votes&group=all)<br/>
* [List_of_datasets_for_machine_learning_research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)<br/>

## Software and Programming

Students are not expected to know any particular language or set of software. We will be demonstrating best practices as used in the Machine Learning for Social Science Lab at the Center for Peace and Security Studies, UCSD. In that lab, our software stack consists of Python and R for data preparation and analysis, Spark for database management, Keras/Tensorflow for deep learning, Github for revision control, and Ubuntu for our operating system and command-line tools.

* [MACS 305001 - Computing for the Social Sciences](https://cfss.uchicago.edu/index.html),Benjamin Soltoff
* ["R for Data Science"](http://r4ds.had.co.nz/), Garrett Grolemund
* ["Spark and sparklyr,"](https://cfss.uchicago.edu/distrib003_spark.html)
* ["GitHub and RStudio,"](https://resources.github.com/articles/github-and-rstudio/)
* ["Data Science at the Command Line,"](https://www.datascienceatthecommandline.com/), Jeroen Janssens, February 8, 2018
* ["Data Visualization: A practical introduction"](http://socviz.co/index.html?utm_content=buffer09710&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer),Kieran Healy
* ["Introduction to Validate," https://cran.r-project.org/web/packages/validate/vignettes/introduction.html)
* ["An introduction to regular expressions"](https://www.oreilly.com/ideas/an-introduction-to-regular-expressions), Thomas Nield, December 13, 2017 ,
* [RegExplain]("https://github.com/gadenbuie/regexplain/#readme")
* ["The Plain Person’s Guide to Plain Text Social Science"](http://plain-text.co/), Kieran Healy, 2018-04-28
* "Statistical Data Cleaning with Applications in R"
* [scikit-learn](http://scikit-learn.org/stable/)
* [Guide to SuperLearner](ftp://cran.r-project.org/pub/R/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html), Chris Kennedy, March 16, 2017

## Applications
* ["ViEWS: a political Violence Early-Warning System,"](http://pcr.uu.se/research/views/) <br/>
* ["Safe Disposal of Unexploded WWII Bombs,"](http://staff.math.su.se/hoehle/blog/2018/05/25/uxb.html) Michael Höhle, May 25, 2018, <br/>
* ["Predicting Race and Ethnicity From the Sequence of Characters in a Name,"](https://arxiv.org/pdf/1805.02109.pdf),Gaurav Sood and Suriyan Laohaprapanon, May 8, 2018,  <br/>
* ["ethnicolr: Predict Race and Ethnicity From Name,"](https://github.com/appeler/ethnicolr)<br/>
* Weidmann, Nils B. and Sebastian Schutte. Using Night Lights for the Prediction of Local Wealth. Journal
of Peace Research 54(2).


## Notes on this Guide

This guide is written as an [R notebook](https://bookdown.org/yihui/rmarkdown/notebook.html) using [R-Studio](https://www.rstudio.com/). It renders output as static HTML that you should be able to view on a regular web browser.

```{r}
#install.packages("pacman")
library(pacman)
p_load(infotheo)
p_load(tidyverse)
p_load(ggplot2)
p_load(cowplot)
p_load(mlbench)
p_load(Metrics)

set.seed(123)

```

# Introduction

## What is machine learning?
* (CIML), [Chapter 1,](http://ciml.info/dl/v0_99/ciml-v0_99-ch01.pdf) <br/>
* (WMLW 1.0) "Introduction" <br/>
* (ISL) "Chapter 2 Statistical Learning""

## What isn't machine learning?

### Statistics
* ["Statistical Modeling: The Two Cultures (with comments and a rejoinder by the  author),"](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726) Leo Breiman, Statistical Science, 2001, Vol. 16, No. 3, 199–231 <br/>

### Causal Inference
* Joshua D. Angrist & Jörn-Steffen Pischke, "Mostly Harmless Econometrics An Empiricist's Companion," 2009
* ["Basic Concepts of Statistical Inference for Causal Effects in Experiments and Observational Studies,"](http://www.stat.columbia.edu/~cook/qr33.pdf) Donald B. Rubin, 2004 <br/>
* ["When and How Should One Use Deep Learning for Causal Effect Inference"](https://technionmail-my.sharepoint.com/personal/urishalit_technion_ac_il/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Furishalit_technion_ac_il%2FDocuments%2FPresentations%2FIAS2018%2FIAS2018_2_for_public%2Epdf&parent=%2Fpersonal%2Furishalit_technion_ac_il%2FDocuments%2FPresentations%2FIAS2018&slrid=aaa2759e-909e-5000-fd16-3e33cabf926f)
* ["Comparing Covariate Prioritization via Matching to Machine Learning Methods for Causal Inference using Five Empirical Applications,"](https://arxiv.org/pdf/1805.03743.pdf),Luke Keele, Dylan Small, May 11, 2018, 

# Information Theory

* [Information_theory](https://en.wikipedia.org/wiki/Information_theory)
* ["Visual Information Theory,"](http://colah.github.io/posts/2015-09-Visual-Information/) Christopher Olah, , October 14, 2015, <br/>
* [Shannon Weaver Model](https://en.wikipedia.org/wiki/Shannon%E2%80%93Weaver_model)<br/>
* PRML 1.6 "Information Theory" <br/>
* ["A Mathematical Theory of Communication,"](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), C. E. SHANNON, October 1948,  The Bell System * Technical Journal, Vol. 27, pp. 379–423, 623–656, July, <br/>

# Information Sources $Y$ 

## Random Variables and Distributions
* [Random_variable](https://en.wikipedia.org/wiki/Random_variable)
* [Binomial_distribution](https://en.wikipedia.org/wiki/Binomial_distribution)
* ["Review of Probability Theory"](http://cs229.stanford.edu/section/cs229-prob.pdf), Arian Maleki and Tom Do <br/>
* PRML 2.0 <br/>

## Entropy
* [Entropy (information_theory)](https://en.wikipedia.org/wiki/Entropy_(information_theory))<br/>

* [Package ‘entropy,’ "Estimation of Entropy, Mutual Information and Related Quantities,"](http://strimmerlab.org/software/entropy/), February 19, 2015  <br/>
* [infotheo](https://cran.r-project.org/web/packages/infotheo/index.html), Patrick E. Meyer, 2014-07-26, R Package <br/>
* [Philentropy: Information Theory and Distance Quantification with R.](https://doi.org/10.21105/joss.00765), Drost, (2018), Journal of Open Source Software, 3(26), 765  <br/>

## Zero-Bit source

* Zero-Bit information sources are constants that don't vary.
* Zero-Bit source, Zero-Bit message: No variation and no measurement. <br/>
* Zero-Bit information source, One-Bit message: No variation, but a single measurement with variation between two states. <br/>
* Zero-Bit information source, N-Bit message: No variation, and an arbitrary number of measurements with arbitrary number of states <br/>

* ["Design, Inference, and the Strategic Logic of Suicide Terrorism"](https://pdfs.semanticscholar.org/f192/cf69908c84d92d269ef52c337fa487d6b65e.pdf), SCOTT ASHWORTH, JOSHUA D. CLINTON, ADAM MEIROWITZ, and KRISTOPHER W. RAMSAY, 2008, APSR

## Binary Source ($<=1bit$)

One-Bit information sources are variables that can take on two different states, e.g. a coin flip. Call $Y$ the true state at the source, and $\hat{Y}$ the mental model of the state at the destination.

(IntroMachineLearningWithR) "Chapter 3 Example datasets"
* [Binary_classification](https://en.wikipedia.org/wiki/Binary_classification)


```{r}
library(infotheo)
N <- 699 #Flip a coin N times (Matched to Breast Cancer Dataset Below)
sample_space <- c(1,0) #Heads and Tails
```

### Fair Coin 

A fair coin has equal likelihood of both heads and tails. Estimated entropy is close to the true value of 1 bit.

```{r}
p <- 0.5 #Fair Coin
Y_coin_fair <- sample(sample_space, size = N,  replace = TRUE,  prob = c(p, 1 - p))  
print(table(Y_coin_fair))
print(natstobits(entropy(Y_coin_fair, method="emp")))
```

### Unfair Coin (p=0.8)
An unfair coin is weighted to be more likely to land heads or tails. Estimated entropy is less than a full bit. There is less surprise than from a full fair coin flip.

```{r}
p <- 0.8 #Unfair Coin
Y_coin_unfair <- sample(sample_space, size = N,  replace = TRUE,  prob = c(p, 1 - p))  
print(table(Y_coin_unfair))
print(natstobits(entropy(Y_coin_unfair, method="emp")))
```

### Two Headed Coin
A two headed coin will only ever land one way.
```{r}
p <- 1 #Two headed coin
Y_coin_twoheaded <- sample(sample_space, size = N,  replace = TRUE,  prob = c(p, 1 - p))  
print(table(Y_coin_twoheaded))
print(natstobits(entropy(Y_coin_twoheaded, method="emp")))
```

### [UCI Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
["Breast Cancer"](https://rpubs.com/raviolli77/352956), Raul Eulogio, January 26, 2018
```{r}
data(BreastCancer)
glimpse(BreastCancer)
summary(BreastCancer$Class)
```

```{r}
print(natstobits(entropy(BreastCancer$Class, method="emp")))
```

## Multi-class Sources

### [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris)

```{r}
data(iris)
glimpse(iris)
summary(iris$Species)
print(natstobits(entropy(iris$Species, method="emp")))
```

## Real Valued Sources

### [Boston Housing Data](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)

```{r, echo=F}
#crim per capita crime rate by town
#zn proportion of residential land zoned for lots over 25,000 sq.ft
#indus proportion of non-retail business acres per town
#chas Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
#nox nitric oxides concentration (parts per 10 million)
#rm average number of rooms per dwelling
#age proportion of owner-occupied units built prior to 1940
#dis weighted distances to five Boston employment centres
#rad index of accessibility to radial highways
#tax full-value property-tax rate per USD 10,000
#ptratio pupil-teacher ratio by town
#b 1000(B − 0.63)2 where B is the proportion of blacks by town
#lstat percentage of lower status of the population
#medv median value of owner-occupied homes in USD 1000’s
```

```{r}
data(BostonHousing)
glimpse(BostonHousing)
summary(BostonHousing$medv)
print(natstobits(entropy(discretize(BostonHousing$medv), method="emp")))
```

# Comparing $Y$ and $\hat{Y}$

(IntroMachineLearningWithR) 5.4 Classification performance

## Binary

How should we compare the true reality $Y$ to our mental model of it $\hat{Y}$?

* [Scoring Rules](https://en.wikipedia.org/wiki/Scoring_rule)<br/>
* [Evaluation_of_binary_classifiers](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers)<br/>
* [Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix)<br/>

### Confusion Matrix

```{r}
table(BreastCancer$Class, BreastCancer$Class)
table(BreastCancer$Class, Y_coin_fair)
table(BreastCancer$Class, Y_coin_unfair)
table(BreastCancer$Class, Y_coin_twoheaded)
```
### Accuracy
```{r}
p_load(Metrics)
BreastCancer$Class_binary <- as.numeric(BreastCancer$Class=="malignant")
accuracy(BreastCancer$Class_binary, BreastCancer$Class_binary)
accuracy(BreastCancer$Class_binary, Y_coin_fair)
accuracy(BreastCancer$Class_binary, Y_coin_unfair)
accuracy(BreastCancer$Class_binary, Y_coin_twoheaded)
```

### Precision

* [Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)<br/>
```{r}
p_load(Metrics)
Metrics::precision(BreastCancer$Class_binary,
         BreastCancer$Class_binary)
Metrics::precision(BreastCancer$Class_binary, Y_coin_fair)
Metrics::precision(BreastCancer$Class_binary, Y_coin_unfair)
Metrics::precision(BreastCancer$Class_binary, Y_coin_twoheaded)
```

### Recall

```{r}
p_load(Metrics)
Metrics::recall(BreastCancer$Class_binary,
         BreastCancer$Class_binary)
Metrics::recall(BreastCancer$Class_binary, Y_coin_fair)
Metrics::recall(BreastCancer$Class_binary, Y_coin_unfair)
Metrics::recall(BreastCancer$Class_binary, Y_coin_twoheaded) 
```

### F1
* [F1_score](https://en.wikipedia.org/wiki/F1_score)<br/>
```{r}
#Note Metrics::f1 doesn't give the correct values
p_load(MLmetrics)
F1_Score(BreastCancer$Class_binary,
         BreastCancer$Class_binary)
F1_Score(BreastCancer$Class_binary, Y_coin_fair)
F1_Score(BreastCancer$Class_binary, Y_coin_unfair)
#F1_Score(BreastCancer$Class_binary, Y_coin_twoheaded) 
#Not happy about all 1 prediction
print( (0.3447783*1)/(0.3447783+1)*2 ) #Calculate by hand
```


* [Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)<br/>
* ["Finding Similar Items"](http://infolab.stanford.edu/~ullman/mmds/ch3.pdf), Jure Leskovec, Anand Rajaraman, Jeff Ullman, Chapter 3, [Mining of Massive Datasets](http://www.mmds.org/), 2014<br/>

## Probalistic Predictions
* [Proper Scoring Rules](https://en.wikipedia.org/wiki/Scoring_rule#ProperScoringRules)<br/>

### Log Loss

[Log Loss](http://wiki.fast.ai/index.php/Log_Loss)

```{r}
MLmetrics::LogLoss(BreastCancer$Class_binary,
         BreastCancer$Class_binary)
MLmetrics::LogLoss(BreastCancer$Class_binary, Y_coin_fair)
MLmetrics::LogLoss(BreastCancer$Class_binary, Y_coin_unfair)
MLmetrics::LogLoss(BreastCancer$Class_binary, Y_coin_twoheaded) 
```

### Area Under the Curve
* [Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)<br/>
* [An introduction to ROC analysis](http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf), Tom Fawcett, 2006, Pattern Recongnition Letters<br/>
* [what-does-auc-stand-for-and-what-is-it](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it)<br/>
* [roc-and-precision-recall-with-imbalanced-datasets](https://classeval.wordpress.com/simulation-analysis/roc-and-precision-recall-with-imbalanced-datasets/)

[Measuring classifier performance: a coherent alternative to the area under the ROC curve](http://web.cs.iastate.edu/~cs573x/Notes/hand-article.pdf), David J. Hand, Mach Learn (2009) 77: 103–123

[Generate ROC Curve Charts for Print and Interactive Use](https://cran.r-project.org/web/packages/plotROC/vignettes/examples.html), Michael C Sachs, 2018-06-23
[Illustrated Guide to ROC and AUC](https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/), Raffael Vogler, June 23, 2015

```{r}
#Simulate a probalistic prediction
noised_prediction <- function(prediction){ noised <-runif(N,0,0.5) ; noised[prediction==1] <-noised[prediction==1]+0.5; return(noised)   }
AUC(noised_prediction(BreastCancer$Class_binary), BreastCancer$Class_binary)
AUC(noised_prediction(Y_coin_fair), BreastCancer$Class_binary)
AUC(noised_prediction(Y_coin_unfair), BreastCancer$Class_binary)
AUC(noised_prediction(Y_coin_twoheaded), BreastCancer$Class_binary)
```

```{r}
p_load(plotROC)
set.seed(2529)
D.ex <- rbinom(200, size = 1, prob = .5)
M1 <- rnorm(200, mean = D.ex, sd = .65)
M2 <- rnorm(200, mean = D.ex, sd = 1.5)

test <- data.frame(D = D.ex, D.str = c("Healthy", "Ill")[D.ex + 1], 
                   M1 = M1, M2 = M2, stringsAsFactors = FALSE)
basicplot <- ggplot(test, aes(d = D, m = M1)) + geom_roc(labels = FALSE)
basicplot


D.ex <- rbinom(50, 1, .5)
rocdata <- data.frame(D = c(D.ex, D.ex), 
                   M = c(rnorm(50, mean = D.ex, sd = .4), rnorm(50, mean = D.ex, sd = 1)), 
                   Z = c(rep("A", 50), rep("B", 50)))

ggplot(rocdata, aes(m = M, d = D)) + geom_roc()

devtools::install_github("sachsmc/plotROC")

ggplot(rocdata, aes(m = M, d = D, color = Z)) + geom_roc()


```

###

["Introduction to the precision-recall plot"](https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/), 

### Others
* [Brier_score](https://en.wikipedia.org/wiki/Brier_score)<br/>

### Imbalanced Data

* ["Facing Imbalanced Data Recommendations for the Use of Performance Metrics"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285355/), László A. Jeni,1 Jeffrey F. Cohn,1,2 and Fernando De La Torre, Int Conf Affect Comput Intell Interact Workshops. 2013; 2013: 245–251.

## Multiclass

* [Multiclass_classification](https://en.wikipedia.org/wiki/Multiclass_classification) <br/>
* [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy) <br/>
* Tom Fawcett (2006) “An introduction to ROC analysis”. Pattern Recognition Letters 27, 861–874. DOI: 10.1016/j.patrec.2005.10.010. <br/>
* David J. Hand and Robert J. Till (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning 45(2), p. 171–186. DOI: 10.1023/A:1010920819831. <br/>
* ["pROC: Display and Analyze ROC Curves"](https://web.expasy.org/pROC/), 2018-05-06 <br/>
* [pROC: an open-source package for R and S+ to analyze and compare ROC curves](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-77), Xavier * * Robin, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez and Markus Müller (2011). BMC Bioinformatics, 12, p. 77 <br/>


## Real Valued

* [Mean_squared_error](https://en.wikipedia.org/wiki/Mean_squared_error)
* [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss)

```{r}
y_hat=mean(BostonHousing$medv)
MAE(BostonHousing$medv, y_hat)
MSE(BostonHousing$medv, y_hat)
```

# Transmitters and Receivers
[Function_(mathematics)](https://en.wikipedia.org/wiki/Function_(mathematics))<br/>
* [Inverse_function](https://en.wikipedia.org/wiki/Inverse_function)<br/>

```{r, include=F}
n=100
Y_unif=runif(n=n)
beta_1=.5
X=(Y_unif)/beta_1 #epsilon_gaussian
p1 <- data.frame(X,Y_unif) %>% ggplot(aes(x=Y_unif,y=X)) + geom_point()

epsilon_gaussian=rnorm(n=n, mean = 0, sd = 1)
beta_1=.5
X=(Y_unif)/beta_1 + (Y_unif^2)/beta_1 #epsilon_gaussian
p2 <- data.frame(X,Y_unif) %>% ggplot(aes(x=Y_unif,y=X)) + geom_point()

X= Y_unif > .5
p3 <- data.frame(X,Y_unif) %>% ggplot(aes(x=Y_unif,y=X)) + geom_point()

X= cos(Y_unif)
p4 <- data.frame(X,Y_unif) %>% ggplot(aes(x=Y_unif,y=X)) + geom_point()

plot_grid(p1,p2,p3,p4, labels = c("A", "B", "C", "D"), ncol = 2)

```


## What makes a good receiver?

### Risk Analysis
* PRML 1.5 "Decision Theory" <br/>
* ESL 2.4 Statistical Decision Theory

### Bias-Variance Tradeoff <br/>
* ["Understanding the Bias-Variance Tradeoff,"](http://scott.fortmann-roe.com/docs/BiasVariance.html), Scott Fortmann-Roe,  2012, <br/>

### Overfitting
* [Overfitting](https://en.wikipedia.org/wiki/Overfitting)<br/>
* [Training,_test,_and_validation_sets](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets)<br/>

### No Free Lunch

* WMLW 2.0 "Related Work"
* ["No Free Lunch Theorems for Optimization,"](https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf) David H. Wolpert and William G. Macready, 1997, <br/>

## Ommitted Variable Bias
* [Simpsons Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)<br/>

## Feature Selection / Included Variable Bias
* [Let’s Put Garbage—Can Regressions and Garbage—Can Probits Where They Belong](http://www.columbia.edu/~gjw10/achen04.pdf), Christopher H. Achen, Conflict Management and Peace Science, 2005
* ["The Phantom Menace: Omitted Variable Bias in Econometric Research"](http://www.saramitchell.org/clarke05.pdf), Kevin A. Clarke, 2005, Conflict Management and Peace Science
* ["Achieving Statistical Significance with Covariates and without Transparency"](https://drive.google.com/file/d/0B13GrSdju4CpcjBKMGU5ZmRjN2c/view), Gabriel Lenz, Alexander Sahn, November 27, 2017

* [SCANNING DEAD SALMON IN FMRI MACHINE HIGHLIGHTS RISK OF RED HERRINGS](https://www.wired.com/2009/09/fmrisalmon/)
* [“Do We Really Know the WTO Cures Cancer?”](http://www.stephenchaudoin.com/CHH_Cancer_2014_09_18.pdf) Stephen Chaudoin, Jude Hays and Raymond Hicks, British Journal of Political Science.

* [Feature_selection](https://en.wikipedia.org/wiki/Feature_selection)<br/>
* [Confounding](https://en.wikipedia.org/wiki/Confounding)<br/>
* ["Statistical learning and selective inference"](http://www.pnas.org/content/pnas/112/25/7629.full.pdf) Jonathan Taylora and Robert J. Tibshirani, June 23, 2015<br/>
* ["The garden of forking paths: Why multiple comparisons can be a problem,  when there is no “fishing expedition” or “p-hacking” and the research  was posited ahead of time"](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf), Andrew Gelman and Eric Loken, 14 Nov 2013, Unpublished Manuscript<br/>
* [P-Hacking](https://projects.fivethirtyeight.com/p-hacking/)<br/>

## Feature Engineering
* [Feature_engineering](https://en.wikipedia.org/wiki/Feature_engineering)<br/>

## Bandwidth / Under determination

* [Underdetermined system](https://en.wikipedia.org/wiki/Underdetermined_system)

* [Overdetermined_system](https://en.wikipedia.org/wiki/Overdetermined_system)<br/>

## Curse of Dimensionality

* [Curse_of_dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) <br/>
* [The Curse of Dimensionality in classification](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/), Vincent Spruyt <br/>
(PRML) "1.4 The Curse of Dimensionality"

